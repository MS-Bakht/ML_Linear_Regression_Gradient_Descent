{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35d2771f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3079feac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradiantDescent:\n",
    "    def __init__(self, X, Y,alpha, itrations = 100):\n",
    "        self.x = X\n",
    "        self.y = Y\n",
    "        self.alpha = alpha\n",
    "        self.itrations = itrations\n",
    "        self.predicted_thetas_final = []\n",
    "        \n",
    "    def hypothesis(self,x,Thetas):\n",
    "        return np.dot(x,Thetas.T)\n",
    "    \n",
    "    def loss_function(self,y_pred,):\n",
    "        return np.mean((self.y - y_pred) **2)\n",
    "    \n",
    "    def get_thetas(self):\n",
    "        Thetas = np.random.rand(self.x.shape[1])\n",
    "        return Thetas \n",
    "        #return np.array([1,1.2]) For verification example given below, comment out the above and uncomment this line\n",
    "        \n",
    "    def derivative_loss_function(self,thetas):\n",
    "        h = self.hypothesis(self.x,thetas)\n",
    "        difference = self.y - h \n",
    "        r = np.dot(difference.T, self.x)\n",
    "        r = -2 * r / float( self.x.shape[0])\n",
    "        return r\n",
    "    \n",
    "    def update_thetas(self,old_thetas,gradiant_thetas):\n",
    "        return old_thetas - ( self.alpha * gradiant_thetas)\n",
    "    \n",
    "    def training(self):\n",
    "        thetas = self.get_thetas()\n",
    "        for i in range(self.itrations):\n",
    "            g_t = self.derivative_loss_function(thetas) #Gradiant thetas\n",
    "            thetas = self.update_thetas(thetas, g_t)\n",
    "            h1 = self.hypothesis(self.x,thetas)\n",
    "            print(self.loss_function(h1))\n",
    "        self.predicted_thetas_final = thetas\n",
    "        \n",
    "    def test(self,tbp):\n",
    "        predicted =  self.hypothesis ( tbp , self.predicted_thetas_final )\n",
    "        print(predicted)\n",
    "        return predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a8875f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example 1:\n",
    "\n",
    "#|X   |   Y  |\n",
    "#|-----------|\n",
    "#| 1  |   7  |\n",
    "#| 3  |   16 | \n",
    "#| 9  |   47 |\n",
    "#| 12 |   61 |\n",
    "#| 14 |   72 |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "86f899dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=np.array([[1,3,9,12,14],[1,1,1,1,1]]).T\n",
    "tbp = np.array([[37],[1]]).T\n",
    "Y=np.array([7,16,47,61,72])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d190a360",
   "metadata": {},
   "outputs": [],
   "source": [
    "obj = GradiantDescent(X,Y,0.003, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c005fe3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "444.69858133430745\n",
      "102.03139970308065\n",
      "23.556865743121868\n",
      "5.585322746232923\n",
      "1.469623061416026\n",
      "0.527062327637707\n",
      "0.3111850117921201\n",
      "0.26172617765369155\n",
      "0.2503790585583746\n",
      "0.24776000863430708\n",
      "0.24713985301428026\n",
      "0.24697753718613774\n",
      "0.24692014264975062\n",
      "0.24688684698023158\n",
      "0.24685914071536613\n",
      "0.24683278474025502\n",
      "0.24680680800567584\n",
      "0.2467809878858525\n",
      "0.24675527315298948\n",
      "0.24672965183213785\n",
      "0.2467041209385354\n",
      "0.24667867954711897\n",
      "0.24665332720535563\n",
      "0.24662806356978262\n",
      "0.24660288832273988\n",
      "0.24657780115331973\n",
      "0.24655280175299604\n",
      "0.2465278898146162\n",
      "0.24650306503216926\n",
      "0.2464783271007284\n",
      "0.24645367571643692\n",
      "0.24642911057651137\n",
      "0.24640463137921537\n",
      "0.24638023782387508\n",
      "0.2463559296108661\n",
      "0.24633170644161034\n",
      "0.2463075680185826\n",
      "0.24628351404529236\n",
      "0.2462595442262807\n",
      "0.24623565826713628\n",
      "0.24621185587446556\n",
      "0.24618813675591045\n",
      "0.2461645006201259\n",
      "0.24614094717679533\n",
      "0.246117476136615\n",
      "0.24609408721129036\n",
      "0.2460707801135349\n",
      "0.24604755455707883\n",
      "0.24602441025663593\n",
      "0.24600134692793207\n",
      "0.245978364287678\n",
      "0.24595546205358687\n",
      "0.24593263994434947\n",
      "0.2459098976796435\n",
      "0.24588723498013026\n",
      "0.24586465156744047\n",
      "0.24584214716419028\n",
      "0.2458197214939581\n",
      "0.24579737428129245\n",
      "0.2457751052517069\n",
      "0.24575291413166864\n",
      "0.2457308006486083\n",
      "0.24570876453090795\n",
      "0.24568680550790262\n",
      "0.24566492330986653\n",
      "0.24564311766802835\n",
      "0.24562138831454985\n",
      "0.24559973498253046\n",
      "0.24557815740600444\n",
      "0.24555665531993748\n",
      "0.24553522846022363\n",
      "0.24551387656367551\n",
      "0.24549259936803022\n",
      "0.24547139661194461\n",
      "0.2454502680349866\n",
      "0.24542921337763332\n",
      "0.24540823238127668\n",
      "0.24538732478820582\n",
      "0.24536649034161648\n",
      "0.2453457287855997\n",
      "0.24532503986514786\n",
      "0.24530442332613553\n",
      "0.24528387891533426\n",
      "0.24526340638040045\n",
      "0.24524300546986968\n",
      "0.24522267593315755\n",
      "0.2452024175205656\n",
      "0.24518222998325773\n",
      "0.24516211307326935\n",
      "0.24514206654351342\n",
      "0.2451220901477534\n",
      "0.24510218364062525\n",
      "0.2450823467776178\n",
      "0.24506257931507874\n",
      "0.2450428810102065\n",
      "0.24502325162104407\n",
      "0.2450036909064936\n",
      "0.24498419862628323\n",
      "0.24496477454099824\n",
      "0.2449454184120535\n"
     ]
    }
   ],
   "source": [
    "obj.training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "44670a3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5.01716087 1.42619997]\n"
     ]
    }
   ],
   "source": [
    "print(obj.predicted_thetas_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "11dd6d5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[187.06115218]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([187.06115218])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj.test(tbp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

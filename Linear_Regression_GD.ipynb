{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35d2771f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3079feac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression:\n",
    "    def __init__(self, X, Y,alpha, itrations = 100):\n",
    "        self.x = X\n",
    "        self.y = Y\n",
    "        self.alpha = alpha\n",
    "        self.itrations = itrations\n",
    "        self.predicted_thetas_final = []\n",
    "        \n",
    "    def hypothesis(self,x,Thetas):\n",
    "        return np.dot(x,Thetas.T)\n",
    "    \n",
    "    def loss_function(self,y_pred,):\n",
    "        return np.mean((self.y - y_pred) **2)\n",
    "    \n",
    "    def get_thetas(self):\n",
    "        Thetas = np.random.rand(self.x.shape[1])\n",
    "        return Thetas \n",
    "        #return np.array([1,1.2]) For verification example given below, comment out the above and uncomment this line\n",
    "        \n",
    "    def derivative_loss_function(self,thetas):\n",
    "        h = self.hypothesis(self.x,thetas)\n",
    "        difference = self.y - h \n",
    "        r = np.dot(difference.T, self.x)\n",
    "        r = -2 * r / float( self.x.shape[0])\n",
    "        return r\n",
    "    \n",
    "    def Gradiant_Descent(self,old_thetas,gradiant_thetas):\n",
    "        return old_thetas - ( self.alpha * gradiant_thetas)\n",
    "    \n",
    "    def training(self):\n",
    "        thetas = self.get_thetas()\n",
    "        for i in range(self.itrations):\n",
    "            g_t = self.derivative_loss_function(thetas) #Gradiant thetas\n",
    "            thetas = self.Gradiant_Descent(thetas, g_t)\n",
    "            h1 = self.hypothesis(self.x,thetas)\n",
    "            print(self.loss_function(h1))\n",
    "        self.predicted_thetas_final = thetas\n",
    "        \n",
    "    def test(self,tbp):\n",
    "        predicted =  self.hypothesis ( tbp , self.predicted_thetas_final )\n",
    "        print(predicted)\n",
    "        return predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a8875f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example 1:\n",
    "\n",
    "#|X   |   Y  |\n",
    "#|-----------|\n",
    "#| 1  |   7  |\n",
    "#| 3  |   16 | \n",
    "#| 9  |   47 |\n",
    "#| 12 |   61 |\n",
    "#| 14 |   72 |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86f899dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=np.array([[1,3,9,12,14],[1,1,1,1,1]]).T\n",
    "tbp = np.array([[37],[1]]).T\n",
    "Y=np.array([7,16,47,61,72])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d190a360",
   "metadata": {},
   "outputs": [],
   "source": [
    "obj = LinearRegression(X,Y,0.003, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c005fe3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "328.72562126852347\n",
      "75.49257785920312\n",
      "17.49936381026998\n",
      "4.218195331389705\n",
      "1.1765721016719355\n",
      "0.47991646521481357\n",
      "0.32028424000286676\n",
      "0.28363653573530245\n",
      "0.27515394043618885\n",
      "0.27312177596757736\n",
      "0.2725671436672321\n",
      "0.2723511945372365\n",
      "0.27221311867335507\n",
      "0.2720931866812331\n",
      "0.27197771879495486\n",
      "0.2718635811183584\n",
      "0.27175005487798476\n",
      "0.27163697439351925\n",
      "0.27152430065275357\n",
      "0.27141202365558875\n",
      "0.27130014004975084\n",
      "0.27118864800910625\n",
      "0.2710775460607222\n",
      "0.27096683281624023\n",
      "0.27085650691036267\n",
      "0.2707465669867144\n",
      "0.270637011694636\n",
      "0.2705278396884072\n",
      "0.2704190496270612\n",
      "0.2703106401743395\n",
      "0.27020260999865325\n",
      "0.27009495777308423\n",
      "0.26998768217534896\n",
      "0.2698807818877927\n",
      "0.26977425559737245\n",
      "0.26966810199563235\n",
      "0.2695623197787023\n",
      "0.2694569076472645\n",
      "0.2693518643065585\n",
      "0.26924718846634504\n",
      "0.269142878840901\n",
      "0.26903893414899976\n",
      "0.2689353531139045\n",
      "0.2688321344633369\n",
      "0.2687292769294717\n",
      "0.26862677924891976\n",
      "0.268524640162717\n",
      "0.26842285841629354\n",
      "0.2683214327594814\n",
      "0.268220361946471\n",
      "0.26811964473582794\n",
      "0.26801927989044855\n",
      "0.2679192661775597\n",
      "0.2678196023687087\n",
      "0.2677202872397299\n",
      "0.26762131957074875\n",
      "0.2675226981461546\n",
      "0.2674244217545881\n",
      "0.2673264891889344\n",
      "0.2672288992462971\n",
      "0.26713165072798617\n",
      "0.2670347424395081\n",
      "0.2669381731905528\n",
      "0.2668419417949617\n",
      "0.26674604707074273\n",
      "0.26665048784002626\n",
      "0.26655526292906945\n",
      "0.26646037116823196\n",
      "0.26636581139197213\n",
      "0.26627158243881677\n",
      "0.2661776831513653\n",
      "0.26608411237625973\n",
      "0.26599086896418195\n",
      "0.265897951769832\n",
      "0.26580535965191665\n",
      "0.2657130914731331\n",
      "0.2656211461001665\n",
      "0.26552952240365624\n",
      "0.2654382192582043\n",
      "0.2653472355423371\n",
      "0.2652565701385134\n",
      "0.26516622193309836\n",
      "0.26507618981635495\n",
      "0.2649864726824259\n",
      "0.26489706942932734\n",
      "0.2648079789589261\n",
      "0.2647192001769371\n",
      "0.2646307319928942\n",
      "0.26454257332015435\n",
      "0.2644547230758717\n",
      "0.2643671801809944\n",
      "0.2642799435602393\n",
      "0.2641930121420847\n",
      "0.2641063848587667\n",
      "0.26402006064624933\n",
      "0.2639340384442167\n",
      "0.2638483171960727\n",
      "0.26376289584891077\n",
      "0.2636777733535061\n",
      "0.26359294866430993\n"
     ]
    }
   ],
   "source": [
    "obj.training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "44670a3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5.03075073 1.27652301]\n"
     ]
    }
   ],
   "source": [
    "print(obj.predicted_thetas_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "11dd6d5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[187.41429999]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([187.41429999])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj.test(tbp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
